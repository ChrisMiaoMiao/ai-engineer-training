import os
import time
from typing import List, Dict, Any
from dotenv import load_dotenv

# LlamaIndex æ ¸å¿ƒç»„ä»¶
from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader, Document
from llama_index.core.node_parser import SentenceSplitter, TokenTextSplitter, SentenceWindowNodeParser
from llama_index.core.schema import TextNode
from llama_index.core.postprocessor import MetadataReplacementPostProcessor

# LLM å’Œ Embedding æ¨¡å‹
from llama_index.llms.openai_like import OpenAILike
from llama_index.embeddings.dashscope import DashScopeEmbedding, DashScopeTextEmbeddingModels



# 1. åŠ è½½ç¯å¢ƒå˜é‡å’Œåˆå§‹åŒ–å…¨å±€é…ç½®
load_dotenv()


# é…ç½® LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰- ä½¿ç”¨é˜¿é‡Œäº‘é€šä¹‰åƒé—®
Settings.llm = OpenAILike(
    model="qwen-plus",  # ä½¿ç”¨é€šä¹‰åƒé—® Plus æ¨¡å‹
    api_base="https://dashscope.aliyuncs.com/compatible-mode/v1/",
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    is_chat_model=True  # æŒ‡å®šä¸ºå¯¹è¯æ¨¡å‹
)

# é…ç½® Embedding æ¨¡å‹ï¼ˆæ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼‰- ç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
Settings.embed_model = DashScopeEmbedding(
    model=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V3,  # ä½¿ç”¨ V3 ç‰ˆæœ¬çš„åµŒå…¥æ¨¡å‹
    embed_batch_size=6,  # æ‰¹å¤„ç†å¤§å°ï¼Œæ¯æ¬¡å¤„ç† 6 ä¸ªæ–‡æœ¬
    embed_input_length=8192  # æœ€å¤§è¾“å…¥é•¿åº¦ä¸º 8192 tokens
)

# é…ç½®æ–‡æœ¬åˆ‡ç‰‡å™¨ï¼ˆç”¨äº SentenceWindowNodeParserï¼‰
# è¿™ä¸ªè®¾ç½®ä¼šè¢« SentenceWindowNodeParser å†…éƒ¨ä½¿ç”¨æ¥åˆ†å‰²æ–‡æœ¬
Settings.text_splitter = SentenceSplitter(
    chunk_size=1024,  # åŸºç¡€åˆ†å¥å™¨çš„å—å¤§å°
    chunk_overlap=20  # åŸºç¡€åˆ†å¥å™¨çš„é‡å å¤§å°
)


# ============================================================
# 2. ä»æ–‡ä»¶åŠ è½½æµ‹è¯•æ–‡æ¡£
# ============================================================
def load_documents_from_data_folder() -> List[Document]:
    """
    ä» data æ–‡ä»¶å¤¹åŠ è½½æµ‹è¯•æ–‡æ¡£
    
    è¿™ä¸ªå‡½æ•°ä½¿ç”¨ SimpleDirectoryReader ä» data æ–‡ä»¶å¤¹ä¸­è¯»å–æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶ï¼Œ
    å¹¶å°†å®ƒä»¬è½¬æ¢ä¸º LlamaIndex çš„ Document å¯¹è±¡ã€‚
    
    Returns:
        åŒ…å«ä»æ–‡ä»¶åŠ è½½çš„æ–‡æ¡£åˆ—è¡¨
    
    Raises:
        ValueError: å¦‚æœ data æ–‡ä»¶å¤¹ä¸å­˜åœ¨æˆ–ä¸ºç©º
    """
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
    current_dir = os.path.dirname(os.path.abspath(__file__))
    
    # æ„å»º data æ–‡ä»¶å¤¹çš„ç»å¯¹è·¯å¾„
    data_dir = os.path.join(current_dir, "data")
    
    # æ£€æŸ¥ data æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_dir):
        raise ValueError(f"âŒ data æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {data_dir}\nè¯·å…ˆåˆ›å»º data æ–‡ä»¶å¤¹å¹¶æ·»åŠ æµ‹è¯•æ–‡æ¡£")
    
    # æ£€æŸ¥ data æ–‡ä»¶å¤¹æ˜¯å¦ä¸ºç©º
    if not os.listdir(data_dir):
        raise ValueError(f"âŒ data æ–‡ä»¶å¤¹ä¸ºç©º: {data_dir}\nè¯·æ·»åŠ è‡³å°‘ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£")
    
    print(f"âœ“ æ­£åœ¨ä» data æ–‡ä»¶å¤¹åŠ è½½æ–‡æ¡£: {data_dir}")
    
    # ä½¿ç”¨ SimpleDirectoryReader è¯»å–æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡æ¡£
    # SimpleDirectoryReader ä¼šè‡ªåŠ¨è¯†åˆ«å¤šç§æ–‡ä»¶æ ¼å¼ï¼ˆ.txt, .pdf, .docx ç­‰ï¼‰
    reader = SimpleDirectoryReader(
        input_dir=data_dir,
        recursive=True,  # é€’å½’è¯»å–å­æ–‡ä»¶å¤¹
        required_exts=[".txt"]  # åªè¯»å– .txt æ–‡ä»¶ï¼Œå¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹
    )
    
    # åŠ è½½æ‰€æœ‰æ–‡æ¡£
    documents = reader.load_data()
    
    # æ‰“å°æ–‡æ¡£ä¿¡æ¯
    print(f"âœ“ æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
    for i, doc in enumerate(documents, 1):
        # è·å–æ–‡ä»¶å
        filename = doc.metadata.get('file_name', 'Unknown')
        # è®¡ç®—æ–‡æ¡£å­—ç¬¦æ•°
        char_count = len(doc.text)
        # ä¼°ç®—è¯æ•°ï¼ˆä¸­æ–‡æŒ‰å­—ç¬¦æ•°ï¼Œè‹±æ–‡æŒ‰ç©ºæ ¼åˆ†å‰²ï¼‰
        word_count = len(doc.text.split()) if doc.text.strip() else 0
        
        print(f"  æ–‡æ¡£ {i}: {filename}")
        print(f"    - å­—ç¬¦æ•°: {char_count:,}")
        print(f"    - ä¼°ç®—è¯æ•°: {word_count:,}")
        print(f"    - å‰100å­—ç¬¦é¢„è§ˆ: {doc.text[:100].strip()}...")
    
    return documents


# ============================================================
# 3. è¯„ä¼°å‡½æ•° - æµ‹è¯•ä¸åŒçš„åˆ‡ç‰‡å‚æ•°ï¼ˆé€šç”¨ç‰ˆæœ¬ï¼‰
# ============================================================
def evaluate_splitter(
    splitter,  # å¯ä»¥æ˜¯ SentenceSplitter æˆ– TokenTextSplitter
    documents: List[Document],
    query: str,
    config_name: str
) -> Dict[str, Any]:
    """
    è¯„ä¼°ç‰¹å®šåˆ‡ç‰‡å™¨é…ç½®çš„æ€§èƒ½ï¼ˆæ”¯æŒå¥å­åˆ‡ç‰‡å’Œ Token åˆ‡ç‰‡ï¼‰
    
    Args:
        splitter: åˆ‡ç‰‡å™¨å®ä¾‹ï¼ˆSentenceSplitter æˆ– TokenTextSplitterï¼‰
        documents: è¦ç´¢å¼•çš„æ–‡æ¡£åˆ—è¡¨
        query: æµ‹è¯•æŸ¥è¯¢é—®é¢˜
        config_name: é…ç½®åç§°ï¼ˆç”¨äºæ ‡è¯†ï¼‰
    
    Returns:
        åŒ…å«è¯„ä¼°ç»“æœçš„å­—å…¸
    """
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•é…ç½®: {config_name}")
    print(f"{'='*60}")
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # 1. ä½¿ç”¨å¥å­åˆ‡ç‰‡å™¨å°†æ–‡æ¡£åˆ†å‰²æˆèŠ‚ç‚¹ï¼ˆchunksï¼‰
    nodes = splitter.get_nodes_from_documents(documents)
    print(f"âœ“ æ–‡æ¡£åˆ‡ç‰‡å®Œæˆ: ç”Ÿæˆäº† {len(nodes)} ä¸ªæ–‡æœ¬å—ï¼ˆchunksï¼‰")
    
    # æ˜¾ç¤ºå‰ 3 ä¸ªèŠ‚ç‚¹çš„ä¿¡æ¯
    print(f"\nå‰ 3 ä¸ªæ–‡æœ¬å—ç¤ºä¾‹:")
    for i, node in enumerate(nodes[:3]):
        print(f"\n  å— #{i+1}:")
        print(f"  - é•¿åº¦: {len(node.text)} å­—ç¬¦")
        print(f"  - é¢„è§ˆ: {node.text[:100]}...")
    
    # 2. ä»èŠ‚ç‚¹åˆ›å»ºå‘é‡ç´¢å¼•
    print(f"\nâœ“ å¼€å§‹åˆ›å»ºå‘é‡ç´¢å¼•...")
    index = VectorStoreIndex(nodes)
    print(f"âœ“ å‘é‡ç´¢å¼•åˆ›å»ºå®Œæˆ")
    
    # 3. åˆ›å»ºæŸ¥è¯¢å¼•æ“
    query_engine = index.as_query_engine(
        similarity_top_k=3  # æ£€ç´¢æœ€ç›¸ä¼¼çš„ 3 ä¸ªæ–‡æœ¬å—
    )
    
    # 4. æ‰§è¡ŒæŸ¥è¯¢
    print(f"\nâœ“ æ‰§è¡ŒæŸ¥è¯¢: '{query}'")
    response = query_engine.query(query)
    
    # è®°å½•ç»“æŸæ—¶é—´
    end_time = time.time()
    elapsed_time = end_time - start_time
    
    # 5. æ˜¾ç¤ºç»“æœ
    print(f"\nã€æŸ¥è¯¢ç»“æœã€‘")
    print(f"å›ç­”: {response.response}")
    print(f"\nã€æ£€ç´¢åˆ°çš„æºæ–‡æœ¬å—ã€‘")
    for i, source_node in enumerate(response.source_nodes):
        print(f"\n  ç›¸å…³æ–‡æœ¬å— #{i+1} (ç›¸ä¼¼åº¦åˆ†æ•°: {source_node.score:.4f}):")
        print(f"  {source_node.text[:200]}...")
    
    # 6. è¿”å›è¯„ä¼°æŒ‡æ ‡
    results = {
        "config_name": config_name,
        "num_chunks": len(nodes),
        "query_time": elapsed_time,
        "response": response.response,
        "num_sources": len(response.source_nodes),
        "avg_similarity": sum(node.score for node in response.source_nodes) / len(response.source_nodes) if response.source_nodes else 0
    }
    
    print(f"\nâ±ï¸  æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
    print(f"ğŸ“Š å¹³å‡ç›¸ä¼¼åº¦åˆ†æ•°: {results['avg_similarity']:.4f}")
    
    return results


# ============================================================
# 3.2 è¯„ä¼°å‡½æ•° - ä¸“é—¨ç”¨äºå¥å­çª—å£åˆ‡ç‰‡
# ============================================================
def evaluate_sentence_window_splitter(
    splitter: SentenceWindowNodeParser,
    documents: List[Document],
    query: str,
    config_name: str
) -> Dict[str, Any]:
    """
    è¯„ä¼°å¥å­çª—å£åˆ‡ç‰‡å™¨çš„æ€§èƒ½
    
    å¥å­çª—å£åˆ‡ç‰‡çš„ç‰¹ç‚¹ï¼š
    - å°†æ–‡æ¡£æŒ‰å¥å­åˆ‡åˆ†ï¼Œæ¯ä¸ªå¥å­ä½œä¸ºä¸€ä¸ªèŠ‚ç‚¹
    - åœ¨å…ƒæ•°æ®ä¸­ä¿å­˜å‘¨å›´å¥å­çš„ä¸Šä¸‹æ–‡çª—å£
    - æ£€ç´¢æ—¶åªç”¨å•å¥åšåŒ¹é…ï¼Œä½†è¿”å›æ—¶å¯ä»¥åŒ…å«å‘¨å›´ä¸Šä¸‹æ–‡
    - è¿™ç§æ–¹æ³•ç»“åˆäº†ç²¾ç¡®æ£€ç´¢å’Œä¸°å¯Œä¸Šä¸‹æ–‡çš„ä¼˜åŠ¿
    
    Args:
        splitter: å¥å­çª—å£åˆ‡ç‰‡å™¨å®ä¾‹
        documents: è¦ç´¢å¼•çš„æ–‡æ¡£åˆ—è¡¨
        query: æµ‹è¯•æŸ¥è¯¢é—®é¢˜
        config_name: é…ç½®åç§°ï¼ˆç”¨äºæ ‡è¯†ï¼‰
    
    Returns:
        åŒ…å«è¯„ä¼°ç»“æœçš„å­—å…¸
    """
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•é…ç½®: {config_name}")
    print(f"{'='*60}")
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # å…³é”®æ­¥éª¤ï¼šå…ˆç”¨ SentenceSplitter æ‰‹åŠ¨åˆ†å‰²æ–‡æ¡£ä¸ºåŸºç¡€èŠ‚ç‚¹
    # è¿™æ ·å¯ä»¥é¿å… SentenceWindowNodeParser å°†æ•´ä¸ªæ–‡æ¡£å½“ä½œä¸€ä¸ªå¥å­
    print(f"âœ“ æ­¥éª¤1: ä½¿ç”¨ SentenceSplitter é¢„å¤„ç†æ–‡æ¡£...")
    base_splitter = SentenceSplitter(
        chunk_size=512,  # åŸºç¡€åˆ†å‰²çš„å—å¤§å°
        chunk_overlap=50  # åŸºç¡€åˆ†å‰²çš„é‡å å¤§å°
    )
    base_nodes = base_splitter.get_nodes_from_documents(documents)
    print(f"âœ“ é¢„å¤„ç†å®Œæˆ: ç”Ÿæˆäº† {len(base_nodes)} ä¸ªåŸºç¡€æ–‡æœ¬å—")
    
    # åœ¨åŸºç¡€èŠ‚ç‚¹ä¸Šåº”ç”¨çª—å£ç­–ç•¥
    print(f"âœ“ æ­¥éª¤2: åœ¨åŸºç¡€èŠ‚ç‚¹ä¸Šæ„å»ºå¥å­çª—å£...")
    nodes = splitter.build_window_nodes_from_documents(base_nodes)
    print(f"âœ“ çª—å£æ„å»ºå®Œæˆ: ç”Ÿæˆäº† {len(nodes)} ä¸ªçª—å£èŠ‚ç‚¹")
    
    # æ˜¾ç¤ºå‰ 3 ä¸ªèŠ‚ç‚¹çš„ä¿¡æ¯
    print(f"\nå‰ 3 ä¸ªå¥å­çª—å£èŠ‚ç‚¹ç¤ºä¾‹:")
    for i, node in enumerate(nodes[:3]):
        print(f"\n  çª—å£èŠ‚ç‚¹ #{i+1}:")
        print(f"  - æ ¸å¿ƒæ–‡æœ¬é•¿åº¦: {len(node.text)} å­—ç¬¦")
        print(f"  - æ ¸å¿ƒæ–‡æœ¬: {node.text[:100]}...")
        # æ˜¾ç¤ºçª—å£ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if 'window' in node.metadata:
            window_text = node.metadata['window']
            print(f"  - çª—å£ä¸Šä¸‹æ–‡é•¿åº¦: {len(window_text)} å­—ç¬¦")
            print(f"  - çª—å£ä¸Šä¸‹æ–‡é¢„è§ˆ: {window_text[:150]}...")
    
    # 2. ä»èŠ‚ç‚¹åˆ›å»ºå‘é‡ç´¢å¼•
    print(f"\nâœ“ å¼€å§‹åˆ›å»ºå‘é‡ç´¢å¼•...")
    index = VectorStoreIndex(nodes)
    print(f"âœ“ å‘é‡ç´¢å¼•åˆ›å»ºå®Œæˆ")
    
    # 3. åˆ›å»ºæŸ¥è¯¢å¼•æ“ï¼Œä½¿ç”¨ MetadataReplacementPostProcessor
    # è¿™ä¸ªåå¤„ç†å™¨ä¼šç”¨çª—å£ä¸Šä¸‹æ–‡æ›¿æ¢æ£€ç´¢åˆ°çš„èŠ‚ç‚¹æ–‡æœ¬
    query_engine = index.as_query_engine(
        similarity_top_k=3,  # æ£€ç´¢æœ€ç›¸ä¼¼çš„ 3 ä¸ªå¥å­èŠ‚ç‚¹
        node_postprocessors=[
            MetadataReplacementPostProcessor(target_metadata_key="window")
        ]
    )
    
    # 4. æ‰§è¡ŒæŸ¥è¯¢
    print(f"\nâœ“ æ‰§è¡ŒæŸ¥è¯¢: '{query}'")
    response = query_engine.query(query)
    
    # è®°å½•ç»“æŸæ—¶é—´
    end_time = time.time()
    elapsed_time = end_time - start_time
    
    # 5. æ˜¾ç¤ºç»“æœ
    print(f"\nã€æŸ¥è¯¢ç»“æœã€‘")
    print(f"å›ç­”: {response.response}")
    print(f"\nã€æ£€ç´¢åˆ°çš„æºæ–‡æœ¬ï¼ˆåŒ…å«çª—å£ä¸Šä¸‹æ–‡ï¼‰ã€‘")
    for i, source_node in enumerate(response.source_nodes):
        print(f"\n  ç›¸å…³å¥å­ #{i+1} (ç›¸ä¼¼åº¦åˆ†æ•°: {source_node.score:.4f}):")
        print(f"  {source_node.text[:300]}...")
    
    # 6. è¿”å›è¯„ä¼°æŒ‡æ ‡
    results = {
        "config_name": config_name,
        "num_chunks": len(nodes),
        "query_time": elapsed_time,
        "response": response.response,
        "num_sources": len(response.source_nodes),
        "avg_similarity": sum(node.score for node in response.source_nodes) / len(response.source_nodes) if response.source_nodes else 0
    }
    
    print(f"\nâ±ï¸  æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
    print(f"ğŸ“Š å¹³å‡ç›¸ä¼¼åº¦åˆ†æ•°: {results['avg_similarity']:.4f}")
    
    return results


# ============================================================
# 4. ä¸»å‡½æ•° - æµ‹è¯•ä¸åŒçš„å‚æ•°ç»„åˆ
# ============================================================
def main():
    """
    ä¸»æµ‹è¯•å‡½æ•°ï¼šæµ‹è¯•ä¸åŒçš„å¥å­åˆ‡ç‰‡å‚æ•°å¯¹æ£€ç´¢æ•ˆæœçš„å½±å“
    """
    # éªŒè¯ API Key
    dashscope_api_key = os.getenv("DASHSCOPE_API_KEY")
    if not dashscope_api_key:
        print("âŒ é”™è¯¯: DASHSCOPE_API_KEY æœªè®¾ç½®ï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®")
        return
    
    print(f"âœ“ DashScope API Key å·²åŠ è½½")
    print(f"\n{'#'*80}")
    print(f"# LlamaIndex æ–‡æœ¬åˆ‡ç‰‡å‚æ•°å½±å“æµ‹è¯•")
    print(f"# åŒ…æ‹¬ï¼šå¥å­åˆ‡ç‰‡ï¼ˆSentenceSplitterï¼‰å’Œ Token åˆ‡ç‰‡ï¼ˆTokenTextSplitterï¼‰")
    print(f"{'#'*80}\n")
    
    # ä» data æ–‡ä»¶å¤¹åŠ è½½æµ‹è¯•æ–‡æ¡£
    try:
        documents = load_documents_from_data_folder()
    except ValueError as e:
        print(str(e))
        return
    
    print(f"\nâœ“ æ–‡æ¡£åŠ è½½å®Œæˆï¼Œå…± {len(documents)} ä¸ªæ–‡æ¡£")
    
    # å®šä¹‰æµ‹è¯•æŸ¥è¯¢
    test_query = "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿå®ƒä¸æœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ"
    print(f"âœ“ æµ‹è¯•é—®é¢˜: {test_query}")
    
    # ============================================================
    # ç¬¬ä¸€éƒ¨åˆ†ï¼šæµ‹è¯•å¥å­åˆ‡ç‰‡ï¼ˆSentenceSplitterï¼‰å‚æ•°é…ç½®
    # ============================================================
    print(f"\n{'*'*80}")
    print(f"* ç¬¬ä¸€éƒ¨åˆ†ï¼šå¥å­åˆ‡ç‰‡ï¼ˆSentenceSplitterï¼‰æµ‹è¯•")
    print(f"* è¯´æ˜ï¼šæŒ‰ç…§å¥å­è¾¹ç•Œè¿›è¡Œåˆ‡ç‰‡ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§")
    print(f"{'*'*80}")
    
    # å¥å­åˆ‡ç‰‡å‚æ•°è¯´æ˜:
    # - chunk_size: æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§å­—ç¬¦æ•°
    # - chunk_overlap: ç›¸é‚»æ–‡æœ¬å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°
    # - paragraph_separator: æ®µè½åˆ†éš”ç¬¦
    
    sentence_configurations = [
        {
            "name": "å¥å­åˆ‡ç‰‡-é…ç½®1: å°å— + æ— é‡å ",
            "chunk_size": 256,
            "chunk_overlap": 0
        },
        {
            "name": "å¥å­åˆ‡ç‰‡-é…ç½®2: å°å— + å°é‡å ",
            "chunk_size": 256,
            "chunk_overlap": 50
        },
        {
            "name": "å¥å­åˆ‡ç‰‡-é…ç½®3: ä¸­ç­‰å— + ä¸­ç­‰é‡å ",
            "chunk_size": 512,
            "chunk_overlap": 50
        },
        {
            "name": "å¥å­åˆ‡ç‰‡-é…ç½®4: ä¸­ç­‰å— + å¤§é‡å ",
            "chunk_size": 512,
            "chunk_overlap": 128
        },
        {
            "name": "å¥å­åˆ‡ç‰‡-é…ç½®5: å¤§å— + ä¸­ç­‰é‡å ",
            "chunk_size": 1024,
            "chunk_overlap": 100
        },
    ]
    
    # å­˜å‚¨æ‰€æœ‰æµ‹è¯•ç»“æœ
    all_results = []
    
    # éå†æ¯ä¸ªå¥å­åˆ‡ç‰‡é…ç½®è¿›è¡Œæµ‹è¯•
    for config in sentence_configurations:
        # åˆ›å»ºå¥å­åˆ‡ç‰‡å™¨
        splitter = SentenceSplitter(
            chunk_size=config["chunk_size"],
            chunk_overlap=config["chunk_overlap"],
            paragraph_separator="\n\n",  # ä½¿ç”¨åŒæ¢è¡Œä½œä¸ºæ®µè½åˆ†éš”ç¬¦
        )
        
        # æ‰§è¡Œè¯„ä¼°
        result = evaluate_splitter(
            splitter=splitter,
            documents=documents,
            query=test_query,
            config_name=config["name"]
        )
        
        all_results.append(result)
        
        # åœ¨æµ‹è¯•ä¹‹é—´ç¨ä½œå»¶è¿Ÿï¼Œé¿å… API é™æµ
        time.sleep(2)
    
    # ============================================================
    # ç¬¬äºŒéƒ¨åˆ†ï¼šæµ‹è¯• Token åˆ‡ç‰‡ï¼ˆTokenTextSplitterï¼‰å‚æ•°é…ç½®
    # ============================================================
    print(f"\n\n{'*'*80}")
    print(f"* ç¬¬äºŒéƒ¨åˆ†ï¼šToken åˆ‡ç‰‡ï¼ˆTokenTextSplitterï¼‰æµ‹è¯•")
    print(f"* è¯´æ˜ï¼šæŒ‰ç…§ Token æ•°é‡è¿›è¡Œåˆ‡ç‰‡ï¼Œæ›´ç²¾ç¡®åœ°æ§åˆ¶æ–‡æœ¬å—å¤§å°")
    print(f"{'*'*80}")
    
    # Token åˆ‡ç‰‡å‚æ•°è¯´æ˜:
    # - chunk_size: æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§ Token æ•°é‡
    # - chunk_overlap: ç›¸é‚»æ–‡æœ¬å—ä¹‹é—´çš„é‡å  Token æ•°é‡
    # - separator: Token åˆ†éš”ç¬¦ï¼ˆé»˜è®¤ä¸ºç©ºæ ¼ï¼‰
    #
    # æ³¨æ„ï¼šToken åˆ‡ç‰‡ä¸å¥å­åˆ‡ç‰‡çš„ä¸»è¦åŒºåˆ«ï¼š
    # 1. Token åˆ‡ç‰‡æŒ‰ token æ•°é‡åˆ‡åˆ†ï¼Œæ›´ç²¾ç¡®åœ°æ§åˆ¶å¤§å°
    # 2. å¥å­åˆ‡ç‰‡å°Šé‡å¥å­è¾¹ç•Œï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§
    # 3. Token åˆ‡ç‰‡é€‚åˆå¯¹é•¿åº¦æœ‰ä¸¥æ ¼è¦æ±‚çš„åœºæ™¯ï¼ˆå¦‚ API é™åˆ¶ï¼‰
    # 4. å¥å­åˆ‡ç‰‡é€‚åˆä¿æŒä¸Šä¸‹æ–‡å®Œæ•´æ€§çš„åœºæ™¯ï¼ˆå¦‚é—®ç­”ç³»ç»Ÿï¼‰
    
    token_configurations = [
        {
            "name": "Tokenåˆ‡ç‰‡-é…ç½®1: å°å—(128 tokens) + æ— é‡å ",
            "chunk_size": 128,
            "chunk_overlap": 0
        },
        {
            "name": "Tokenåˆ‡ç‰‡-é…ç½®2: å°å—(128 tokens) + å°é‡å (20 tokens)",
            "chunk_size": 128,
            "chunk_overlap": 20
        },
        {
            "name": "Tokenåˆ‡ç‰‡-é…ç½®3: ä¸­ç­‰å—(256 tokens) + ä¸­ç­‰é‡å (30 tokens)",
            "chunk_size": 256,
            "chunk_overlap": 30
        },
        {
            "name": "Tokenåˆ‡ç‰‡-é…ç½®4: ä¸­ç­‰å—(256 tokens) + å¤§é‡å (50 tokens)",
            "chunk_size": 256,
            "chunk_overlap": 50
        },
        {
            "name": "Tokenåˆ‡ç‰‡-é…ç½®5: å¤§å—(512 tokens) + ä¸­ç­‰é‡å (50 tokens)",
            "chunk_size": 512,
            "chunk_overlap": 50
        },
    ]
    
    # éå†æ¯ä¸ª Token åˆ‡ç‰‡é…ç½®è¿›è¡Œæµ‹è¯•
    for config in token_configurations:
        # åˆ›å»º Token åˆ‡ç‰‡å™¨
        # TokenTextSplitter ä½¿ç”¨åˆ†è¯å™¨å°†æ–‡æœ¬åˆ†å‰²æˆ tokensï¼Œç„¶åæŒ‰æŒ‡å®šå¤§å°åˆ‡ç‰‡
        splitter = TokenTextSplitter(
            chunk_size=config["chunk_size"],
            chunk_overlap=config["chunk_overlap"],
            separator=" ",  # ä½¿ç”¨ç©ºæ ¼ä½œä¸ºåŸºæœ¬åˆ†éš”ç¬¦
        )
        
        # æ‰§è¡Œè¯„ä¼°
        result = evaluate_splitter(
            splitter=splitter,
            documents=documents,
            query=test_query,
            config_name=config["name"]
        )
        
        all_results.append(result)
        
        # åœ¨æµ‹è¯•ä¹‹é—´ç¨ä½œå»¶è¿Ÿï¼Œé¿å… API é™æµ
        time.sleep(2)
    
    # ============================================================
    # ç¬¬ä¸‰éƒ¨åˆ†ï¼šæµ‹è¯•å¥å­çª—å£åˆ‡ç‰‡ï¼ˆSentenceWindowNodeParserï¼‰å‚æ•°é…ç½®
    # ============================================================
    print(f"\n\n{'*'*80}")
    print(f"* ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¥å­çª—å£åˆ‡ç‰‡ï¼ˆSentenceWindowNodeParserï¼‰æµ‹è¯•")
    print(f"* è¯´æ˜ï¼šå°†æ–‡æ¡£æŒ‰å¥å­åˆ‡åˆ†ï¼Œæ¯ä¸ªå¥å­ä¿å­˜å‘¨å›´å¥å­ä½œä¸ºä¸Šä¸‹æ–‡çª—å£")
    print(f"*       æ£€ç´¢æ—¶ç”¨å•å¥åŒ¹é…ï¼Œè¿”å›æ—¶åŒ…å«çª—å£ä¸Šä¸‹æ–‡ï¼Œå…¼é¡¾ç²¾ç¡®æ€§å’Œå®Œæ•´æ€§")
    print(f"{'*'*80}")
    
    # å¥å­çª—å£åˆ‡ç‰‡å‚æ•°è¯´æ˜:
    # - window_size: çª—å£å¤§å°ï¼Œå³åœ¨æ ¸å¿ƒå¥å­å‰åå„ä¿ç•™å¤šå°‘ä¸ªå¥å­
    # - window_metadata_key: å­˜å‚¨çª—å£ä¸Šä¸‹æ–‡çš„å…ƒæ•°æ®é”®å
    # - original_text_metadata_key: å­˜å‚¨åŸå§‹å¥å­çš„å…ƒæ•°æ®é”®å
    #
    # å·¥ä½œåŸç†ï¼š
    # 1. ä½¿ç”¨ Settings.text_splitter (SentenceSplitter) å°†æ–‡æ¡£æŒ‰å¥å­åˆ‡åˆ†
    # 2. ä¸ºæ¯ä¸ªå¥å­èŠ‚ç‚¹ä¿å­˜å‰å N ä¸ªå¥å­ä½œä¸ºä¸Šä¸‹æ–‡çª—å£
    # 3. å‘é‡åŒ–æ—¶åªå¯¹æ ¸å¿ƒå¥å­è¿›è¡ŒåµŒå…¥ï¼ˆä¿è¯æ£€ç´¢ç²¾ç¡®æ€§ï¼‰
    # 4. è¿”å›ç»“æœæ—¶ç”¨çª—å£ä¸Šä¸‹æ–‡æ›¿æ¢æ ¸å¿ƒå¥å­ï¼ˆæä¾›å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰
    #
    # ä¼˜åŠ¿ï¼š
    # - æ£€ç´¢ç²¾ç¡®ï¼šåªåŒ¹é…æ ¸å¿ƒå¥å­ï¼Œä¸å—æ— å…³ä¸Šä¸‹æ–‡å¹²æ‰°
    # - ä¸Šä¸‹æ–‡ä¸°å¯Œï¼šè¿”å›æ—¶åŒ…å«å‘¨å›´å¥å­ï¼Œä¾¿äºç†è§£
    # - é€‚åˆé—®ç­”ï¼šå¯ä»¥ç²¾ç¡®å®šä½ç­”æ¡ˆå¥ï¼ŒåŒæ—¶æä¾›è¶³å¤Ÿçš„èƒŒæ™¯ä¿¡æ¯
    
    sentence_window_configurations = [
        {
            "name": "å¥å­çª—å£-é…ç½®1: çª—å£å¤§å°=1ï¼ˆå‰åå„1å¥ï¼‰",
            "window_size": 1
        },
        {
            "name": "å¥å­çª—å£-é…ç½®2: çª—å£å¤§å°=2ï¼ˆå‰åå„2å¥ï¼‰",
            "window_size": 2
        },
        {
            "name": "å¥å­çª—å£-é…ç½®3: çª—å£å¤§å°=3ï¼ˆå‰åå„3å¥ï¼‰",
            "window_size": 3
        },
        {
            "name": "å¥å­çª—å£-é…ç½®4: çª—å£å¤§å°=5ï¼ˆå‰åå„5å¥ï¼‰",
            "window_size": 5
        },
        {
            "name": "å¥å­çª—å£-é…ç½®5: çª—å£å¤§å°=10ï¼ˆå‰åå„10å¥ï¼‰",
            "window_size": 10
        },
    ]
    
    # éå†æ¯ä¸ªå¥å­çª—å£é…ç½®è¿›è¡Œæµ‹è¯•
    for config in sentence_window_configurations:
        # åˆ›å»ºå¥å­çª—å£åˆ‡ç‰‡å™¨
        # æ³¨æ„ï¼šSentenceWindowNodeParser ä¼šä½¿ç”¨ Settings.text_splitter æ¥åˆ†å‰²æ–‡æœ¬
        splitter = SentenceWindowNodeParser.from_defaults(
            window_size=config["window_size"],
            window_metadata_key="window",  # çª—å£ä¸Šä¸‹æ–‡å­˜å‚¨åœ¨ 'window' å…ƒæ•°æ®ä¸­
            original_text_metadata_key="original_sentence"  # åŸå§‹å¥å­å­˜å‚¨é”®
        )
        
        # æ‰§è¡Œè¯„ä¼°ï¼ˆä½¿ç”¨ä¸“é—¨çš„å¥å­çª—å£è¯„ä¼°å‡½æ•°ï¼‰
        result = evaluate_sentence_window_splitter(
            splitter=splitter,
            documents=documents,
            query=test_query,
            config_name=config["name"]
        )
        
        all_results.append(result)
        
        # åœ¨æµ‹è¯•ä¹‹é—´ç¨ä½œå»¶è¿Ÿï¼Œé¿å… API é™æµ
        time.sleep(2)
    
    # ============================================================
    # è¾“å‡ºæ€»ç»“æŠ¥å‘Š
    # ============================================================
    print(f"\n\n{'='*100}")
    print(f"æµ‹è¯•æ€»ç»“æŠ¥å‘Š - å¥å­åˆ‡ç‰‡ vs Token åˆ‡ç‰‡ vs å¥å­çª—å£åˆ‡ç‰‡ ä¸‰æ–¹å¯¹æ¯”")
    print(f"{'='*100}\n")
    
    print(f"{'é…ç½®åç§°':<60} | {'æ–‡æœ¬å—æ•°':<10} | {'æŸ¥è¯¢è€—æ—¶(ç§’)':<14} | {'å¹³å‡ç›¸ä¼¼åº¦':<12}")
    print(f"{'-'*60}-+-{'-'*10}-+-{'-'*14}-+-{'-'*12}")
    
    for result in all_results:
        print(f"{result['config_name']:<60} | {result['num_chunks']:<10} | "
              f"{result['query_time']:<14.2f} | {result['avg_similarity']:<12.4f}")
    
    # åˆ†åˆ«æ‰¾å‡ºä¸‰ç§åˆ‡ç‰‡æ–¹æ³•çš„æœ€ä½³é…ç½®
    sentence_results = [r for r in all_results if "å¥å­åˆ‡ç‰‡-" in r['config_name']]
    token_results = [r for r in all_results if "Tokenåˆ‡ç‰‡" in r['config_name']]
    window_results = [r for r in all_results if "å¥å­çª—å£" in r['config_name']]
    
    print(f"\n{'='*100}")
    print(f"æœ€ä½³é…ç½®åˆ†æ")
    print(f"{'='*100}")
    
    if sentence_results:
        best_sentence = max(sentence_results, key=lambda x: x['avg_similarity'])
        print(f"\nğŸ† å¥å­åˆ‡ç‰‡æœ€ä½³é…ç½®: {best_sentence['config_name']}")
        print(f"   - å¹³å‡ç›¸ä¼¼åº¦åˆ†æ•°: {best_sentence['avg_similarity']:.4f}")
        print(f"   - ç”Ÿæˆæ–‡æœ¬å—æ•°: {best_sentence['num_chunks']}")
        print(f"   - æŸ¥è¯¢è€—æ—¶: {best_sentence['query_time']:.2f} ç§’")
    
    if token_results:
        best_token = max(token_results, key=lambda x: x['avg_similarity'])
        print(f"\nğŸ† Token åˆ‡ç‰‡æœ€ä½³é…ç½®: {best_token['config_name']}")
        print(f"   - å¹³å‡ç›¸ä¼¼åº¦åˆ†æ•°: {best_token['avg_similarity']:.4f}")
        print(f"   - ç”Ÿæˆæ–‡æœ¬å—æ•°: {best_token['num_chunks']}")
        print(f"   - æŸ¥è¯¢è€—æ—¶: {best_token['query_time']:.2f} ç§’")
    
    if window_results:
        best_window = max(window_results, key=lambda x: x['avg_similarity'])
        print(f"\nğŸ† å¥å­çª—å£åˆ‡ç‰‡æœ€ä½³é…ç½®: {best_window['config_name']}")
        print(f"   - å¹³å‡ç›¸ä¼¼åº¦åˆ†æ•°: {best_window['avg_similarity']:.4f}")
        print(f"   - ç”Ÿæˆæ–‡æœ¬å—æ•°: {best_window['num_chunks']}")
        print(f"   - æŸ¥è¯¢è€—æ—¶: {best_window['query_time']:.2f} ç§’")
    
    # æ€»ä½“æœ€ä½³é…ç½®
    overall_best = max(all_results, key=lambda x: x['avg_similarity'])
    print(f"\nğŸ¯ æ€»ä½“æœ€ä½³é…ç½®: {overall_best['config_name']}")
    print(f"   - å¹³å‡ç›¸ä¼¼åº¦åˆ†æ•°: {overall_best['avg_similarity']:.4f}")
    print(f"   - ç”Ÿæˆæ–‡æœ¬å—æ•°: {overall_best['num_chunks']}")
    print(f"   - æŸ¥è¯¢è€—æ—¶: {overall_best['query_time']:.2f} ç§’")
    
    # å…³é”®æ´å¯Ÿ
    print(f"\n{'='*100}")
    print(f"å…³é”®æ´å¯Ÿ")
    print(f"{'='*100}")
    
    avg_sentence_chunks = sum(r['num_chunks'] for r in sentence_results) / len(sentence_results) if sentence_results else 0
    avg_token_chunks = sum(r['num_chunks'] for r in token_results) / len(token_results) if token_results else 0
    avg_window_chunks = sum(r['num_chunks'] for r in window_results) / len(window_results) if window_results else 0
    
    avg_sentence_similarity = sum(r['avg_similarity'] for r in sentence_results) / len(sentence_results) if sentence_results else 0
    avg_token_similarity = sum(r['avg_similarity'] for r in token_results) / len(token_results) if token_results else 0
    avg_window_similarity = sum(r['avg_similarity'] for r in window_results) / len(window_results) if window_results else 0
    
    print(f"\nğŸ“Š å¥å­åˆ‡ç‰‡ç»Ÿè®¡:")
    print(f"   - å¹³å‡ç”Ÿæˆæ–‡æœ¬å—æ•°: {avg_sentence_chunks:.1f}")
    print(f"   - å¹³å‡ç›¸ä¼¼åº¦åˆ†æ•°: {avg_sentence_similarity:.4f}")
    
    print(f"\nğŸ“Š Token åˆ‡ç‰‡ç»Ÿè®¡:")
    print(f"   - å¹³å‡ç”Ÿæˆæ–‡æœ¬å—æ•°: {avg_token_chunks:.1f}")
    print(f"   - å¹³å‡ç›¸ä¼¼åº¦åˆ†æ•°: {avg_token_similarity:.4f}")
    
    print(f"\nğŸ“Š å¥å­çª—å£åˆ‡ç‰‡ç»Ÿè®¡:")
    print(f"   - å¹³å‡ç”Ÿæˆæ–‡æœ¬å—æ•°: {avg_window_chunks:.1f}")
    print(f"   - å¹³å‡ç›¸ä¼¼åº¦åˆ†æ•°: {avg_window_similarity:.4f}")
    
    print(f"\nğŸ’¡ å»ºè®®:")
    
    # æ‰¾å‡ºè¡¨ç°æœ€å¥½çš„æ–¹æ³•
    method_scores = {
        "å¥å­åˆ‡ç‰‡": avg_sentence_similarity,
        "Token åˆ‡ç‰‡": avg_token_similarity,
        "å¥å­çª—å£åˆ‡ç‰‡": avg_window_similarity
    }
    best_method = max(method_scores, key=method_scores.get)
    
    if best_method == "å¥å­åˆ‡ç‰‡":
        print(f"   âœ¨ å¥å­åˆ‡ç‰‡åœ¨æœ¬æ¬¡æµ‹è¯•ä¸­è¡¨ç°æœ€å¥½ï¼ˆå¹³å‡ç›¸ä¼¼åº¦: {avg_sentence_similarity:.4f}ï¼‰")
        print(f"   ğŸ“Œ å¥å­åˆ‡ç‰‡èƒ½æ›´å¥½åœ°ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ï¼Œé€‚åˆé—®ç­”å’Œå¯¹è¯ç³»ç»Ÿã€‚")
        print(f"   ğŸ“Œ é€‚ç”¨åœºæ™¯ï¼šéœ€è¦ä¿æŒå¥å­å®Œæ•´æ€§å’Œä¸Šä¸‹æ–‡è¿è´¯æ€§çš„åº”ç”¨")
    elif best_method == "Token åˆ‡ç‰‡":
        print(f"   âœ¨ Token åˆ‡ç‰‡åœ¨æœ¬æ¬¡æµ‹è¯•ä¸­è¡¨ç°æœ€å¥½ï¼ˆå¹³å‡ç›¸ä¼¼åº¦: {avg_token_similarity:.4f}ï¼‰")
        print(f"   ğŸ“Œ Token åˆ‡ç‰‡èƒ½æ›´ç²¾ç¡®åœ°æ§åˆ¶æ–‡æœ¬å—å¤§å°ï¼Œé€‚åˆæœ‰ä¸¥æ ¼é•¿åº¦é™åˆ¶çš„åœºæ™¯ã€‚")
        print(f"   ğŸ“Œ é€‚ç”¨åœºæ™¯ï¼šAPI token é™åˆ¶ã€æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£é™åˆ¶ç­‰")
    else:
        print(f"   âœ¨ å¥å­çª—å£åˆ‡ç‰‡åœ¨æœ¬æ¬¡æµ‹è¯•ä¸­è¡¨ç°æœ€å¥½ï¼ˆå¹³å‡ç›¸ä¼¼åº¦: {avg_window_similarity:.4f}ï¼‰")
        print(f"   ğŸ“Œ å¥å­çª—å£åˆ‡ç‰‡å…¼é¡¾æ£€ç´¢ç²¾ç¡®æ€§å’Œä¸Šä¸‹æ–‡å®Œæ•´æ€§ï¼Œæ˜¯ä¸€ç§å¹³è¡¡æ–¹æ¡ˆã€‚")
        print(f"   ğŸ“Œ é€‚ç”¨åœºæ™¯ï¼šé—®ç­”ç³»ç»Ÿã€ä¿¡æ¯æ£€ç´¢ã€éœ€è¦ç²¾ç¡®å®šä½ä½†åˆè¦æä¾›å……è¶³ä¸Šä¸‹æ–‡çš„åœºæ™¯")
        print(f"   ğŸ“Œ ç‰¹åˆ«æ¨èï¼šå½“ç­”æ¡ˆå¯èƒ½åœ¨å•ä¸ªå¥å­ä¸­ï¼Œä½†éœ€è¦å‘¨å›´å¥å­æ‰èƒ½å®Œå…¨ç†è§£æ—¶")
    
    # æä¾›ç»¼åˆå»ºè®®
    print(f"\nğŸ¯ ç»¼åˆå»ºè®®:")
    print(f"   - å¦‚æœéœ€è¦ç²¾ç¡®æ£€ç´¢å•ä¸ªæ¦‚å¿µæˆ–äº‹å® â†’ æ¨èå¥å­çª—å£åˆ‡ç‰‡ï¼ˆçª—å£å¤§å°2-3ï¼‰")
    print(f"   - å¦‚æœéœ€è¦ä¿æŒæ®µè½çº§åˆ«çš„è¯­ä¹‰å®Œæ•´æ€§ â†’ æ¨èå¥å­åˆ‡ç‰‡ï¼ˆå—å¤§å°512-1024ï¼‰")
    print(f"   - å¦‚æœæœ‰ä¸¥æ ¼çš„ token æ•°é‡é™åˆ¶ â†’ æ¨è Token åˆ‡ç‰‡ï¼ˆæ ¹æ®é™åˆ¶è°ƒæ•´å—å¤§å°ï¼‰")
    print(f"   - å¦‚æœæ–‡æ¡£ç»“æ„å¤æ‚ä¸”ç­”æ¡ˆåˆ†æ•£ â†’ å»ºè®®æµ‹è¯•å¤šç§æ–¹æ³•å¹¶ç»“åˆä½¿ç”¨")
    
    print(f"\n{'='*100}")
    print(f"æµ‹è¯•å®Œæˆï¼")
    print(f"{'='*100}")


if __name__ == "__main__":
    main()
